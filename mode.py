# -*- coding: utf-8 -*-
"""Intership1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cGytOz_yCh5beyg7KxZgDjVvOuZi9vBR
"""

# Lets import some packages
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
import re
import random
import spacy
import nltk
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from collections import Counter
import json
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from tqdm.notebook import tqdm
from transformers import BertTokenizer, AutoTokenizer
from torch.utils.data import TensorDataset
from transformers import BertForSequenceClassification, AutoModel, AutoModelForSequenceClassification
from transformers import AdamW, get_linear_schedule_with_warmup
from transformers import TextClassificationPipeline
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.utils import shuffle
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import gc

def f1_score_func(preds, labels):
    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, preds_flat, average='weighted')


def accuracy_per_class(preds, labels):
    label_dict = {}
    for index, possible_label in enumerate(train_df['category'].unique()):
        label_dict[possible_label] = index

    label_dict_inverse = {v: k for k, v in label_dict.items()}

    preds_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()

    for label in np.unique(labels_flat):
        y_preds = preds_flat[labels_flat==label]
        y_true = labels_flat[labels_flat==label]
        print(f'Class: {label_dict_inverse[label]}')
        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\n')

def evaluate(dataloader_val):

    model.eval()

    loss_val_total = 0
    predictions, true_vals = [], []

    for batch in dataloader_val:

        batch = tuple(b.to(device) for b in batch)

        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():
            outputs = model(**inputs)

        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)

    loss_val_avg = loss_val_total/len(dataloader_val)

    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)

    return loss_val_avg, predictions, true_vals

df = pd.read_csv('/content/Data_Train.csv')
len_ = df.shape[0]
#df = df.sample(n=int(len_*0.5)).reset_index(drop=True)
len_ = df.shape[0]
df['text'] = df['title']
df = df.iloc[:,1:]

df_train = df.sample(n=int(len_*0.8), random_state=1).reset_index(drop=True)  # You can change the random_state if desired
df_test = df.drop(df_train.index).reset_index(drop=True)
df_train.shape,df_test.shape

# Splite the data
train_df = df_train.copy()
X_train, X_val, y_train, y_val = train_test_split(train_df.index.values,
                                                  train_df.label.values,
                                                  test_size=0.1,
                                                  random_state=42,
                                                  stratify=train_df.label.values)

# Specify the data_type (train and val)
train_df['data_type'] = ['not_set']*train_df.shape[0]

train_df.loc[X_train, 'data_type'] = 'train'
train_df.loc[X_val, 'data_type'] = 'val'

train_df.groupby(['category', 'label', 'data_type','lang']).count()

"""#BERT"""

# Intiate a bert tokenizer
MODEL_TYPE = 'bert-base-uncased'

tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE,
                                          do_lower_case=True)
# Encode the train dataq
# Truncation is a must scince we know that we have sequences larger than 512
encoded_data_train = tokenizer.batch_encode_plus(
    list(train_df[train_df.data_type=='train'].text.values),
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    truncation = True,
    max_length=512,
    return_tensors='pt'
)

encoded_data_val = tokenizer.batch_encode_plus(
    list(train_df[train_df.data_type=='val'].text.values),
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    truncation = True,
    max_length=512,
    return_tensors='pt'
)



input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(train_df[train_df.data_type=='train'].label.values)

input_ids_val = encoded_data_val['input_ids']
attention_masks_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(train_df[train_df.data_type=='val'].label.values)

# instantiate a bert model
model = BertForSequenceClassification.from_pretrained(MODEL_TYPE,
                                                      num_labels=len(train_df['label'].unique()),
                                                      output_attentions=False,
                                                      output_hidden_states=False)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(device)

# Prepare tensor datasets
dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)
dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)

batch_size = 16 ###BATCH

dataloader_train = DataLoader(dataset_train,
                              sampler=RandomSampler(dataset_train),
                              batch_size=batch_size)

dataloader_validation = DataLoader(dataset_val,
                                   sampler=SequentialSampler(dataset_val),
                                   batch_size=batch_size)
optimizer = AdamW(model.parameters(),
                  lr=1e-5,
                  eps=1e-8)

epochs = 2 ###EPOCH
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,
                                            num_training_steps=len(dataloader_train)*epochs)

# Seeding all
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

for epoch in tqdm(range(1, epochs+1)):

    model.train()

    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()

        batch = tuple(b.to(device) for b in batch)

        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        outputs = model(**inputs)

        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()

        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})


    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')

    tqdm.write(f'\nEpoch {epoch}')

    loss_train_avg = loss_train_total/len(dataloader_train)
    tqdm.write(f'Training loss: {loss_train_avg}')

    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (Weighted): {val_f1}')

model = BertForSequenceClassification.from_pretrained("bert-base-uncased",
                                                      num_labels=len(train_df['label'].unique()),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

model.to(device)

model.load_state_dict(torch.load('finetuned_BERT_epoch_2.model', map_location=torch.device('cpu')))


model.save_pretrained("finetuned_bert_news_model")
tokenizer.save_pretrained("finetuned_bert_news_model_tokenizer")

_, predictions, true_vals = evaluate(dataloader_validation)
accuracy_per_class(predictions, true_vals)

model = AutoModelForSequenceClassification.from_pretrained('./finetuned_bert_news_model')
tokenizer = AutoTokenizer.from_pretrained('./finetuned_bert_news_model_tokenizer', return_tensors = 'pt')
pipe = TextClassificationPipeline(model = model,
                                  tokenizer = tokenizer,
                                  framework = 'pt',
                                  function_to_apply = 'softmax')

preds = []
for row in train_df.iloc[-10:].itertuples():
    article_txt = row.text.split()
    article_txt = article_txt[:300]
    article_txt = ' '.join([w for w in article_txt])
    pred = pipe(article_txt)
    print('raw_title',article_txt,pred)
    print('true_title',row.label)
    preds.append(pred[0]['label'].split('_')[1])

"""#mBERT"""

# Intiate a bert tokenizer
MODEL_TYPE = 'bert-base-multilingual-uncased'

tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE,
                                          do_lower_case=True)
# Encode the train dataq
# Truncation is a must scince we know that we have sequences larger than 512
encoded_data_train = tokenizer.batch_encode_plus(
    list(train_df[train_df.data_type=='train'].text.values),
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    truncation = True,
    max_length=512,
    return_tensors='pt'
)

encoded_data_val = tokenizer.batch_encode_plus(
    list(train_df[train_df.data_type=='val'].text.values),
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    truncation = True,
    max_length=512,
    return_tensors='pt'
)



input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(train_df[train_df.data_type=='train'].label.values)

input_ids_val = encoded_data_val['input_ids']
attention_masks_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(train_df[train_df.data_type=='val'].label.values)

# instantiate a bert model
model = BertForSequenceClassification.from_pretrained(MODEL_TYPE,
                                                      num_labels=len(train_df['label'].unique()),
                                                      output_attentions=False,
                                                      output_hidden_states=False)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(device)

# Prepare tensor datasets
dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)
dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)

batch_size = 16 ###BATCH

dataloader_train = DataLoader(dataset_train,
                              sampler=RandomSampler(dataset_train),
                              batch_size=batch_size)

dataloader_validation = DataLoader(dataset_val,
                                   sampler=SequentialSampler(dataset_val),
                                   batch_size=batch_size)
optimizer = AdamW(model.parameters(),
                  lr=1e-5,
                  eps=1e-8)

epochs = 2 ###EPOCH
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,
                                            num_training_steps=len(dataloader_train)*epochs)

# Seeding all
seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

for epoch in tqdm(range(1, epochs+1)):

    model.train()

    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()

        batch = tuple(b.to(device) for b in batch)

        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        outputs = model(**inputs)

        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()

        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})


    torch.save(model.state_dict(), f'finetuned_mBERT_epoch_{epoch}.model')

    tqdm.write(f'\nEpoch {epoch}')

    loss_train_avg = loss_train_total/len(dataloader_train)
    tqdm.write(f'Training loss: {loss_train_avg}')

    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (Weighted): {val_f1}')

model = BertForSequenceClassification.from_pretrained(MODEL_TYPE,
                                                      num_labels=len(train_df['label'].unique()),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

model.to(device)

model.load_state_dict(torch.load('finetuned_BERT_epoch_2.model', map_location=torch.device('cpu')))


model.save_pretrained("finetuned_mbert_news_model")
tokenizer.save_pretrained("finetuned_mbert_news_model_tokenizer")

_, predictions, true_vals = evaluate(dataloader_validation)
accuracy_per_class(predictions, true_vals)

model = AutoModelForSequenceClassification.from_pretrained('./finetuned_mbert_news_model')
tokenizer = AutoTokenizer.from_pretrained('./finetuned_mbert_news_model_tokenizer', return_tensors = 'pt')
pipe = TextClassificationPipeline(model = model,
                                  tokenizer = tokenizer,
                                  framework = 'pt',
                                  function_to_apply = 'softmax')

preds = []
for row in train_df.iloc[-10:].itertuples():
    article_txt = row.text.split()
    article_txt = article_txt[:300]
    article_txt = ' '.join([w for w in article_txt])
    pred = pipe(article_txt)
    print('raw_title',article_txt,pred)
    print('true_title',row.label)
    preds.append(pred[0]['label'].split('_')[1])

"""#XLM_RoBERTA"""

# Intiate a bert tokenizer
MODEL_TYPE = 'xlm-roberta-base'

tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')

# Encode the train dataq
# Truncation is a must scince we know that we have sequences larger than 512
encoded_data_train = tokenizer.batch_encode_plus(
    list(train_df[train_df.data_type=='train'].text),
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    truncation = True,
    max_length=512,
    return_tensors='pt'
)

encoded_data_val = tokenizer.batch_encode_plus(
    list(train_df[train_df.data_type=='val'].text),
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    truncation = True,
    max_length=512,
    return_tensors='pt'
)


input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(train_df[train_df.data_type=='train'].label.values)

input_ids_val = encoded_data_val['input_ids']
attention_masks_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(train_df[train_df.data_type=='val'].label.values)

# instantiate a bert model
from transformers import AutoTokenizer, XLMRobertaForSequenceClassification

model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_TYPE,
                                                      num_labels=len(train_df['label'].unique()),
                                                      output_attentions=False,
                                                      output_hidden_states=False)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(device)

# Prepare tensor datasets
dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)
dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)

batch_size = 16

dataloader_train = DataLoader(dataset_train,
                              sampler=RandomSampler(dataset_train),
                              batch_size=batch_size)

dataloader_validation = DataLoader(dataset_val,
                                   sampler=SequentialSampler(dataset_val),
                                   batch_size=batch_size)
optimizer = AdamW(model.parameters(),
                  lr=1e-5,
                  eps=1e-8)

epochs = 2
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,
                                            num_training_steps=len(dataloader_train)*epochs)

# Seeding all
seed_val = 64
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

for epoch in tqdm(range(1, epochs+1)):

    model.train()

    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()

        batch = tuple(b.to(device) for b in batch)

        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        outputs = model(**inputs)

        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()

        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})


    torch.save(model.state_dict(), f'finetuned_XLM_RoBERT_epoch_{epoch}.model')

    tqdm.write(f'\nEpoch {epoch}')

    loss_train_avg = loss_train_total/len(dataloader_train)
    tqdm.write(f'Training loss: {loss_train_avg}')

    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (Weighted): {val_f1}')

model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_TYPE,
                                                      num_labels=len(train_df['label'].unique()),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

model.to(device)

model.load_state_dict(torch.load('finetuned_XLM_RoBERT_epoch_2.model', map_location=torch.device('cpu')))


model.save_pretrained("finetuned_XLM_Robert_news_model")
tokenizer.save_pretrained("finetuned_XLM_Robert_news_model_tokenizer")

_, predictions, true_vals = evaluate(dataloader_validation)
accuracy_per_class(predictions, true_vals)

model = AutoModelForSequenceClassification.from_pretrained('./finetuned_XLM_Robert_news_model')
tokenizer = AutoTokenizer.from_pretrained('./finetuned_XLM_Robert_news_model_tokenizer', return_tensors = 'pt')
pipe = TextClassificationPipeline(model = model,
                                  tokenizer = tokenizer,
                                  framework = 'pt',
                                  function_to_apply = 'softmax')

preds = []
for row in train_df.iloc[-10:].itertuples():
    article_txt = row.text.split()
    article_txt = article_txt[:300]
    article_txt = ' '.join([w for w in article_txt])
    pred = pipe(article_txt)
    print('raw_title',article_txt,pred)
    print('true_title',row.label)
    preds.append(pred[0]['label'].split('_')[1])

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

!zip -r XML_Robert.zip finetuned_XLM_Robert_news_model_tokenizer/

!zip -r XML_Robert.zip finetuned_XLM_Robert_news_model/

!zip -r BERT.zip finetuned_bert_news_model_tokenizer/
!zip -r BERT.zip finetuned_bert_news_model/

